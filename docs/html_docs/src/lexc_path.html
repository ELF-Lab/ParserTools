<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>ParserTools.src.lexc_path API documentation</title>
<meta name="description" content="Functions and classes for converting a morphological paradigm
spreadsheet row into one or more paths in a lexc file.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ParserTools.src.lexc_path</code></h1>
</header>
<section id="section-intro">
<p>Functions and classes for converting a morphological paradigm
spreadsheet row into one or more paths in a lexc file.</p>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="ParserTools.src.lexc_path.ALT_TAG"><code class="name">var <span class="ident">ALT_TAG</span></code></dt>
<dd>
<div class="desc"><p>Tag which indicates alternative or non-standard analyses.</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.MAXFORMS"><code class="name">var <span class="ident">MAXFORMS</span></code></dt>
<dd>
<div class="desc"><p>Maximum number of alternate forms for a single analysis in the
spreadsheets.</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.PREFIX_BOUNDARY"><code class="name">var <span class="ident">PREFIX_BOUNDARY</span></code></dt>
<dd>
<div class="desc"><p>Morpheme boundary between prefix and stem.</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.SUFFIX_BOUNDARY"><code class="name">var <span class="ident">SUFFIX_BOUNDARY</span></code></dt>
<dd>
<div class="desc"><p>Morpheme boundary between stem and suffix.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ParserTools.src.lexc_path.entry2str"><code class="name flex">
<span>def <span class="ident">entry2str</span></span>(<span>entry: <a title="ParserTools.src.lexc_path.LexcEntry" href="#ParserTools.src.lexc_path.LexcEntry">LexcEntry</a>) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def entry2str(entry:LexcEntry) -&gt; str:
    &#34;&#34;&#34;Return the string representation of a LexcEntry object. When the
        upper and lower string are identical, the function will
        collapse them into one entry. If the upper and lower string
        are empty, only the continuation lexicon is retained.  This
        enhances readability of the lexc file.

    &#34;&#34;&#34;
    if entry.analysis == entry.surface:
        if entry.analysis == &#34;0&#34;:
            return f&#34;{entry.next_lexicon} ;&#34;
        else:
            return f&#34;{entry.analysis} {entry.next_lexicon} ;&#34;
    else:
        return f&#34;{entry.analysis}:{entry.surface} {entry.next_lexicon} ;&#34;</code></pre>
</details>
<div class="desc"><p>Return the string representation of a LexcEntry object. When the
upper and lower string are identical, the function will
collapse them into one entry. If the upper and lower string
are empty, only the continuation lexicon is retained.
This
enhances readability of the lexc file.</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.escape"><code class="name flex">
<span>def <span class="ident">escape</span></span>(<span>symbol: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def escape(symbol:str) -&gt; str:
    &#34;&#34;&#34;Escape lexc special characters using a `%`. If the character is
        already escaped using `%`, don&#39;t add a second escape
        symbol. This function will escape symbols in the range
        `[!%&lt;&gt;0/#; ]`

    &#34;&#34;&#34;
    return re.sub(&#34;(?&lt;!%)([!%&lt;&gt;0/#; ])&#34;,r&#34;%\1&#34;,symbol)</code></pre>
</details>
<div class="desc"><p>Escape lexc special characters using a <code>%</code>. If the character is
already escaped using <code>%</code>, don't add a second escape
symbol. This function will escape symbols in the range
<code>[!%&lt;&gt;0/#; ]</code></p></div>
</dd>
<dt id="ParserTools.src.lexc_path.split_form"><code class="name flex">
<span>def <span class="ident">split_form</span></span>(<span>form: str) ‑> <a title="ParserTools.src.lexc_path.SplitForm" href="#ParserTools.src.lexc_path.SplitForm">SplitForm</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split_form(form:str) -&gt; SplitForm:
    &#34;&#34;&#34;Split a form `prefix&lt;&lt;stem&gt;&gt;suffix` (e.g. found in the column
        `Form1Split` in paradigm spreadsheets) at morpheme boundaries
        (`&lt;&lt;` and `&gt;&gt;`). If either boundary is missing, the function
        will add one at the start and end and issue a warning. A
        SplitForm object is returned.

    &#34;&#34;&#34;
    # re.split results in a 5-element array [prefix, &#34;&lt;&lt;&#34;, stem, &#34;&gt;&gt;&#34;,
    # suffix]
    if not PREFIX_BOUNDARY in form:
        form = PREFIX_BOUNDARY + form
        warn(f&#34;Invalid segmented form: {form}. Appending morpheme boundary &#39;{PREFIX_BOUNDARY}&#39; at the start.&#34;)    
    if not SUFFIX_BOUNDARY in form:
        form += SUFFIX_BOUNDARY
        warn(f&#34;Invalid segmented form: {form}. Appending morpheme boundary &#39;{SUFFIX_BOUNDARY}&#39; at the end.&#34;)
    form = re.split(f&#34;({PREFIX_BOUNDARY}|{SUFFIX_BOUNDARY})&#34;, form)
    if len(form) != 5:
        raise ValueError(f&#34;Invalid form: {orig_form}. Split: {form}&#34;)
    return SplitForm(escape(form[0]), escape(form[2]), escape(form[4]))</code></pre>
</details>
<div class="desc"><p>Split a form <code>prefix&lt;&lt;stem&gt;&gt;suffix</code> (e.g. found in the column
<code>Form1Split</code> in paradigm spreadsheets) at morpheme boundaries
(<code>&lt;&lt;</code> and <code>&gt;&gt;</code>). If either boundary is missing, the function
will add one at the start and end and issue a warning. A
SplitForm object is returned.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ParserTools.src.lexc_path.DerivationPath"><code class="flex name class">
<span>class <span class="ident">DerivationPath</span></span>
<span>(</span><span>row: pandas.core.series.Series, conf: dict)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DerivationPath:
    def __init__(self, row:pd.core.series.Series, conf:dict):
        self.conf = conf
        self.form = row.Form
        self.tag = f&#34;+{row.Tag}&#34;
        self.input_paradigm = row.InputParadigm
        self.input_class = row.InputClass
        self.output_paradigm = row.OutputParadigm
        self.output_class = row.OutputClass
        LexcPath.multichar_symbols.update([self.tag])
        
    def extend_lexicons(self, lexicons:dict) -&gt; None:
        input_boundary_lexicon = f&#34;{self.input_paradigm}_Class={self.input_class}_Boundary&#34;
        analysis = f&#34;+{self.input_paradigm}{self.tag}&#34;
        surface = f&#34;0{escape(SUFFIX_BOUNDARY)}{self.form}&#34;
        output_boundary_lexicon = f&#34;{self.output_paradigm}_Class={self.output_class}_Boundary&#34;
        if not input_boundary_lexicon in lexicons:
            lexicons[input_boundary_lexicon] = set()
        lexicons[input_boundary_lexicon].add(LexcEntry(input_boundary_lexicon,
                                                       analysis,
                                                       surface,
                                                       output_boundary_lexicon))</code></pre>
</details>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="ParserTools.src.lexc_path.DerivationPath.extend_lexicons"><code class="name flex">
<span>def <span class="ident">extend_lexicons</span></span>(<span>self, lexicons: dict) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extend_lexicons(self, lexicons:dict) -&gt; None:
    input_boundary_lexicon = f&#34;{self.input_paradigm}_Class={self.input_class}_Boundary&#34;
    analysis = f&#34;+{self.input_paradigm}{self.tag}&#34;
    surface = f&#34;0{escape(SUFFIX_BOUNDARY)}{self.form}&#34;
    output_boundary_lexicon = f&#34;{self.output_paradigm}_Class={self.output_class}_Boundary&#34;
    if not input_boundary_lexicon in lexicons:
        lexicons[input_boundary_lexicon] = set()
    lexicons[input_boundary_lexicon].add(LexcEntry(input_boundary_lexicon,
                                                   analysis,
                                                   surface,
                                                   output_boundary_lexicon))</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="ParserTools.src.lexc_path.LexcEntry"><code class="flex name class">
<span>class <span class="ident">LexcEntry</span></span>
<span>(</span><span>lexicon, analysis, surface, next_lexicon)</span>
</code></dt>
<dd>
<div class="desc"><p>LexcEntry represents a Lexc sublexicon entry like:</p>
<pre><code>LEXICON Lex
   upper:lower NextLex ;
</code></pre>
<p>which corresponds to: </p>
<pre><code>entry = LexcEntry(&quot;Lex&quot;, &quot;upper&quot;, &quot;lower&quot;, &quot;NextLex&quot;)
</code></pre>
<p>Access features as:</p>
<pre><code>entry.lexicon, entry.analysis, entry.surface, entry.next_lexicon
</code></pre></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="ParserTools.src.lexc_path.LexcEntry.analysis"><code class="name">var <span class="ident">analysis</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.LexcEntry.lexicon"><code class="name">var <span class="ident">lexicon</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.LexcEntry.next_lexicon"><code class="name">var <span class="ident">next_lexicon</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 3</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.LexcEntry.surface"><code class="name">var <span class="ident">surface</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 2</p></div>
</dd>
</dl>
</dd>
<dt id="ParserTools.src.lexc_path.LexcPath"><code class="flex name class">
<span>class <span class="ident">LexcPath</span></span>
<span>(</span><span>row: pandas.core.series.Series, conf: dict, regular: bool)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LexcPath:
    &#34;&#34;&#34;The LexcPath class represents a path in a lexc file from a root
       lexicon like `VerbRoot` or `NounRoot` to the terminal lexicon
       `#`. This path corresponds to a list of LexcEntry objects: 

       ``` [entry_1, entry_2, ..., entry_n] ``` 

       where `entry_1.lexicon` is the root lexicon, 
       `entry_i+1.lexicon = entry_i.next_lexicon` and
       `entry_n.next_lexicon = &#34;#&#34;`

       A lexc file can be thought of as a union of this type of paths.

       A LexcPath object encodes all the information on one spreadsheet
       row (e.g. a row in VTA_IND.csv in the OjibweMorph repo). Thus
       the path can contain multiple surface forms (Surface1Form,
       Surface2Form, ...). LexcPath can therefore encode multiple
       sub-lexicon entries e.g. corresponding to alternative endings of
       an inflected form.
    &#34;&#34;&#34;

    multichar_symbols:set[str] = set()
    &#34;&#34;&#34;Multichar symbols (across all lexc files) are stored in this static
       set

    &#34;&#34;&#34;

    @classmethod
    def update_multichar_symbol_set(cls, conf:dict) -&gt; None:
        &#34;&#34;&#34;This function adds all multicharacter symbols speficied in a
           configuration file + morpheme boundaries into the
           `multichar_symbols` set.

        &#34;&#34;&#34;
        cls.multichar_symbols.update(map(escape,conf[&#34;multichar_symbols&#34;]))
        cls.multichar_symbols.add(escape(PREFIX_BOUNDARY))
        cls.multichar_symbols.add(escape(SUFFIX_BOUNDARY))
        cls.multichar_symbols.add(escape(ALT_TAG))
        
    @classmethod
    def get_prefix_flags(cls, prefix:str) -&gt; tuple[str]:
        &#34;&#34;&#34;Get P and R flag diacritics like @P.Prefix.NI@ which
           determine valid combinations of prefixes and suffixes.

           The flag diacritics will be added to `multichar_symbols`.

        &#34;&#34;&#34;
        prefix = &#34;NONE&#34; if prefix == &#34;&#34; else prefix.upper()
        pflag, rflag = f&#34;@P.Prefix.{prefix}@&#34;, f&#34;@R.Prefix.{prefix}@&#34;
        cls.multichar_symbols.update([pflag, rflag])
        return pflag, rflag

    @classmethod
    def get_paradigm_flags(cls, paradigm:str) -&gt; tuple[str]:
        &#34;&#34;&#34;Get P and R flag diacritics for a given paradigm like VTA.

           The flag diacritics will be added to `multichar_symbols`.

        &#34;&#34;&#34;        
        pflag, rflag = f&#34;@P.Paradigm.{paradigm}@&#34;, f&#34;@R.Paradigm.{paradigm}@&#34;
        cls.multichar_symbols.update([pflag, rflag])        
        return pflag, rflag
    
    def get_order_flag(self) -&gt; tuple[str]:
        &#34;&#34;&#34;Get a U flag matching the order of this LexcPath: Ind, Cnj or
           Other (in case of imperative or other unspecified order).

           The flag diacritic will be added to `multichar_symbols`.
        &#34;&#34;&#34;
        order = &#34;Other&#34;
        if &#34;+Ind&#34; in self.tags:
            order = &#34;Ind&#34;
        elif &#34;+Cnj&#34; in self.tags:
            order = &#34;Cnj&#34;
        flag = f&#34;@U.Order.{order}@&#34;
        LexcPath.multichar_symbols.update([flag])
        return order, flag

    def __init__(self, row:pd.core.series.Series, conf:dict, regular:bool):
        &#34;&#34;&#34;Initialize this LexcPath using the configuration file conf and a
        spreadsheet row. The boolean parameter regular determines whether this
        is treated as a regular form (which should have morpheme boundaries
        and undergo phonological rules) or an irregular form which is stored
        in the lexc file in verbatim.

        All morphological features like &#34;+VTA&#34;, &#34;+Ind&#34; and &#34;+1SgSubj&#34;
        apperaing on the spreadsheet row will be added to
        multichar_symbols.

        &#34;&#34;&#34;
        self.root_lexicon = conf[&#34;root_lexicon&#34;]
        self.row = row
        self.conf = conf
        self.regular = regular
        self.paradigm = row[&#34;Paradigm&#34;]
        self.klass = row[&#34;Class&#34;]
        self.lemma = escape(row[&#34;Lemma&#34;])
        self.stem = escape(row[&#34;Stem&#34;])
        self.tags = [escape(f&#34;+{row[feat]}&#34;)
                     for feat in conf[&#34;morph_features&#34;]
                     if (row[feat] != conf[&#34;missing_tag_marker&#34;] and
                         row[feat] != &#34;&#34;)]
        self.harvest_multichar_symbols()

        try:
            self.read_forms(row, conf)
        except ValueError as e:
            warn(e, force=False)
            
    def harvest_multichar_symbols(self) -&gt; None:
        &#34;&#34;&#34;Add all morphological features like &#34;+VTA&#34;, &#34;+Ind&#34; and &#34;+1SgSubj&#34;
           from this path to the multichar_symbols set

        &#34;&#34;&#34;
        for tag in self.tags:
            LexcPath.multichar_symbols.add(tag)

    def read_forms(self, row:pd.core.series.Series, conf:dict) -&gt; None:
        &#34;&#34;&#34;Read all forms on the given dataframe row. Store both the plain
           surface form and segmented form.

        &#34;&#34;&#34;
        def get_form_indices() -&gt; list[int]:
            # Return all indices i which are associated with a surface
            # form on this row, i.e. i where Form{i}Surface is a
            # column on the row and the form in that column is
            # non-empty.
            missing = conf[&#34;missing_form_marker&#34;]
            return [i for i in range(MAXFORMS) if f&#34;Form{i}Surface&#34; in row and
                                                  not row[f&#34;Form{i}Surface&#34;] in [missing, &#34;&#34;]]
        
        self.forms = [(row[f&#34;Form{i}Surface&#34;], split_form(row[f&#34;Form{i}Split&#34;]))
                      for i in get_form_indices()]
        if len(self.forms) == 0:
            raise ValueError(f&#34;No surface forms given for row: {row.to_dict()}&#34;)
        
    def get_lexc_paths(self) -&gt; list[list[LexcEntry]]:
        &#34;&#34;&#34;Convert this path into a list of lexc lexicon paths starting
           at a person prefix lexicon (this could be VTA_Prefix, NA_Prefix, 
           etc.) and ending in the terminal lexicon #. Each path is a 
           sequence of lexc sublexicon entries.

           There will be one list-element for each surface form (note that 
           there may be several surface forms Form1Surface, Form2Surface, 
           ...).

           For inflected forms of regular lexemes, our paths will look
           like this (here, for the example analysis and intermediate form
           `aaba&#39;+VTA+Ind+Neg+Dub+0Pl+1Sg:ni&lt;&lt;aaba&#39;w&gt;&gt;igosiinaadogenan`):

           ```
              ! Person prefix lexicon for nouns and verbs. For all other 
              ! word classes the prefix is always empty (@P.Prefix.NONE@)
              LEXICON VTA_Prefix
              @P.Prefix.NI@:@P.Prefix.NI@ni VTA_PrefixBoundary ;

              ! Morpheme boundary for person prefix. Note that we can jump
              ! to a preverb lexicon here. For all word classes apart from
              ! nouns and verbs, we jump directly to the a stem lexicon.
              LEXICON VTA_PrefixBoundary
              0:%&lt;%&lt; PreverbRoot ;
        
              ! After adding preverbs, we return to this lexicon. Need to
              ! match the correct paradigm here
              LEXICON VerbStems
              @R.Paradigm.VTA@ VTA_Stems ;

              ! Stem lexicon. aaba&#39; belongs to the VTA_C inflection class,
              ! so we continue to the VTA_C suffix boundary lexicon.
              LEXICON VTA_Stems
              aaba&#39;:aaba&#39;w VTA_Class=VTA_C_Boundary ;

              ! Suffix boundary
              LEXICON VTA_Class=VTA_C_Boundary
              0:%&gt;%&gt; VTA_Class=VTA_C_Flags ;

              ! This sublexicon makes sure that we get the correct 
              ! combination of person prefix (&#34;ni-&#34; in this case) and ending.
              ! The combinatorics is handled by matching the value of the 
              ! feature Prefix (NI in this case).
              LEXICON VTA_Class=VTA_C_Flags
              @R.Prefix.NI@ VTA_Class=VTA_C_Flags_Prefix=NI ;

              ! We need to match the correct order here
              LEXICON VTA_Class=VTA_C_Flags_Prefix=NI ;
              @U.Order.Ind@ VTA_Class=VTA_C_Prefix=NI_Order=Ind_Endings ;

              ! This lexicon enumerates endings for the inflection class
              ! VAT_C which correspond to person prefix &#34;ni-&#34; and order Ind.
              LEXICON VTA_Class=VTA_C_Prefix=NI_Order=Ind_Endings
              +VTA+Ind+Neg+Dub+%0PlSubj+1SgObj:igosiinaadogenan # ;
           ```

           For inflected forms of irregular lexemes, our paths become
           very simple. We just enumerate the entire form as one
           chunk without morpheme boundaries. This effectively prevents any
           phonological rules from applying, which is exactly what we want
           for irregular lexemes:

           ```
              LEXICON ROOT
              VTA_Irregular ;

              LEXICON VTA_Irregular
              izhi+VTA+Ind+Pos+Neu+%0Pl+1Sg:nindigonan # ;
           ```
        &#34;&#34;&#34;
        paths = []
        paradigm = self.paradigm
        klass = self.klass
        for i, (surface, parts) in enumerate(self.forms):
            tags = deepcopy(self.tags)
            # Possibly append a +Alt tag to all analyses except the
            # first one.
            if i &gt; 0 and self.conf[&#34;append_alt_tag&#34;]:
                tags.append(ALT_TAG)
            if self.regular:
                # Initialize flag diacritics which:
                # (1) control combinations of person prefix and inflectional ending,
                # (2) check that we&#39;ve got the correct paradigm (this is needed to
                #     make sure that return to the correct paradigm after adding
                #     preverbs), and
                # (3) check the order (we track this because subordinate preverbs
                #     and the changed-conjunct marker require conjunct order and some
                #     preverbs have distinct independent and conjunct order surface
                #     forms) 
                set_prefix_flag, check_prefix_flag = LexcPath.get_prefix_flags(parts.prefix)
                _, check_paradigm_flag = LexcPath.get_paradigm_flags(paradigm)
                order, check_order_flag = self.get_order_flag()

                # Initialize the person prefix for this form
                prefix = &#34;NONE&#34; if parts.prefix == &#34;&#34; else parts.prefix.upper()

                # Initialize continuation lexicons needed on this path
                person_prefix_lexicon = f&#34;{paradigm}_Prefix&#34;
                morpheme_boundary_lexicon = f&#34;{paradigm}_PrefixBoundary&#34;
                preverb_lexicon = (self.conf[&#34;prefix_root&#34;] # This can also be the prenoun lexicon depending on paradigm
                                   if &#34;prefix_root&#34; in self.conf
                                   else None)
                pos_stem_lexicon = f&#34;{self.conf[&#39;pos&#39;]}Stems&#34; # E.g. VerbStems
                paradigm_stem_lexicon = f&#34;{paradigm}_Stems&#34; # E.g. VTA_Stems
                inflection_class_lexicon = f&#34;{paradigm}_Class={klass}_Boundary&#34;
                check_prefix_lexicon = f&#34;{paradigm}_Class={klass}_Flags&#34;
                check_order_lexicon = f&#34;{paradigm}_Class={klass}_Flags_Prefix={prefix}&#34;
                ending_lexicon = f&#34;{paradigm}_Class={klass}_Prefix={prefix}_Order={order}_Endings&#34;
                
                path = [
                    LexcEntry(person_prefix_lexicon,
                              set_prefix_flag,
                              set_prefix_flag+parts.prefix,
                              morpheme_boundary_lexicon),
                    LexcEntry(morpheme_boundary_lexicon,
                              &#34;0&#34;,
                              escape(PREFIX_BOUNDARY),
                              preverb_lexicon or pos_stem_lexicon),
                    LexcEntry(pos_stem_lexicon,
                              check_paradigm_flag,
                              check_paradigm_flag,
                              paradigm_stem_lexicon),
                    LexcEntry(paradigm_stem_lexicon,
                              self.lemma,
                              self.stem,
                              inflection_class_lexicon),
                    LexcEntry(inflection_class_lexicon,
                              &#34;0&#34;,
                              escape(SUFFIX_BOUNDARY),
                              check_prefix_lexicon),
                    LexcEntry(check_prefix_lexicon,
                              check_prefix_flag,
                              check_prefix_flag,
                              check_order_lexicon),
                    LexcEntry(check_order_lexicon,
                              check_order_flag,
                              check_order_flag,
                              ending_lexicon),
                    LexcEntry(ending_lexicon,
                              &#34;&#34;.join(tags),
                              parts.suffix,
                              &#34;#&#34;)
                ]
                paths.append(path)
            else:
                # Irregular forms are treated as one chunk and simply enumerated.
                paths.append([LexcEntry(f&#34;{paradigm}_Irregular&#34;,
                                        f&#34;{self.lemma}{&#39;&#39;.join(tags)}&#34;,
                                        surface,
                                        &#34;#&#34;)])

        return paths

    def extend_lexicons(self, lexicons:dict) -&gt; None:
        &#34;&#34;&#34;Add the all lexicon entries on this path to lexicons. The parameter
           lexicons represents all sublexicons in the lexc file and their lexicon
           entries.

        &#34;&#34;&#34;
        def get_paradigm(s):
            return re.sub(&#34;[_].*&#34;,&#34;&#34;,s)
        for path in self.get_lexc_paths():
            paradigm = get_paradigm(path[0].lexicon)
            p_paradigm_flag, _ = LexcPath.get_paradigm_flags(paradigm)
            lexicons[self.root_lexicon].add(LexcEntry(self.root_lexicon,
                                                      p_paradigm_flag,
                                                      p_paradigm_flag,
                                                      path[0].lexicon))
            for lexc_entry in path:
                if not lexc_entry.lexicon in lexicons:
                    lexicons[lexc_entry.lexicon] = set()
                lexicons[lexc_entry.lexicon].add(lexc_entry)
        
    def __str__(self):        
        paths = self.get_lexc_paths()
        res = &#34;&#34;
        for path in paths:
            for lexc_entry in path:
                res += f&#34;{lexc_entry}\n&#34;
            res += &#34;----\n&#34;
        return res

    def __hash__(self):
        return hash(str(self))</code></pre>
</details>
<div class="desc"><p>The LexcPath class represents a path in a lexc file from a root
lexicon like <code>VerbRoot</code> or <code>NounRoot</code> to the terminal lexicon
<code>#</code>. This path corresponds to a list of LexcEntry objects: </p>
<p><code>[entry_1, entry_2, ..., entry_n]</code> </p>
<p>where <code>entry_1.lexicon</code> is the root lexicon,
<code>entry_i+1.lexicon = entry_i.next_lexicon</code> and
<code>entry_n.next_lexicon = "#"</code></p>
<p>A lexc file can be thought of as a union of this type of paths.</p>
<p>A LexcPath object encodes all the information on one spreadsheet
row (e.g. a row in VTA_IND.csv in the OjibweMorph repo). Thus
the path can contain multiple surface forms (Surface1Form,
Surface2Form, &hellip;). LexcPath can therefore encode multiple
sub-lexicon entries e.g. corresponding to alternative endings of
an inflected form.</p>
<p>Initialize this LexcPath using the configuration file conf and a
spreadsheet row. The boolean parameter regular determines whether this
is treated as a regular form (which should have morpheme boundaries
and undergo phonological rules) or an irregular form which is stored
in the lexc file in verbatim.</p>
<p>All morphological features like "+VTA", "+Ind" and "+1SgSubj"
apperaing on the spreadsheet row will be added to
multichar_symbols.</p></div>
<h3>Class variables</h3>
<dl>
<dt id="ParserTools.src.lexc_path.LexcPath.multichar_symbols"><code class="name">var <span class="ident">multichar_symbols</span> : set[str]</code></dt>
<dd>
<div class="desc"><p>Multichar symbols (across all lexc files) are stored in this static
set</p></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="ParserTools.src.lexc_path.LexcPath.get_paradigm_flags"><code class="name flex">
<span>def <span class="ident">get_paradigm_flags</span></span>(<span>paradigm: str) ‑> tuple[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Get P and R flag diacritics for a given paradigm like VTA.</p>
<p>The flag diacritics will be added to <code>multichar_symbols</code>.</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.LexcPath.get_prefix_flags"><code class="name flex">
<span>def <span class="ident">get_prefix_flags</span></span>(<span>prefix: str) ‑> tuple[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Get P and R flag diacritics like @P.Prefix.NI@ which
determine valid combinations of prefixes and suffixes.</p>
<p>The flag diacritics will be added to <code>multichar_symbols</code>.</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.LexcPath.update_multichar_symbol_set"><code class="name flex">
<span>def <span class="ident">update_multichar_symbol_set</span></span>(<span>conf: dict) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>This function adds all multicharacter symbols speficied in a
configuration file + morpheme boundaries into the
<code>multichar_symbols</code> set.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ParserTools.src.lexc_path.LexcPath.extend_lexicons"><code class="name flex">
<span>def <span class="ident">extend_lexicons</span></span>(<span>self, lexicons: dict) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extend_lexicons(self, lexicons:dict) -&gt; None:
    &#34;&#34;&#34;Add the all lexicon entries on this path to lexicons. The parameter
       lexicons represents all sublexicons in the lexc file and their lexicon
       entries.

    &#34;&#34;&#34;
    def get_paradigm(s):
        return re.sub(&#34;[_].*&#34;,&#34;&#34;,s)
    for path in self.get_lexc_paths():
        paradigm = get_paradigm(path[0].lexicon)
        p_paradigm_flag, _ = LexcPath.get_paradigm_flags(paradigm)
        lexicons[self.root_lexicon].add(LexcEntry(self.root_lexicon,
                                                  p_paradigm_flag,
                                                  p_paradigm_flag,
                                                  path[0].lexicon))
        for lexc_entry in path:
            if not lexc_entry.lexicon in lexicons:
                lexicons[lexc_entry.lexicon] = set()
            lexicons[lexc_entry.lexicon].add(lexc_entry)</code></pre>
</details>
<div class="desc"><p>Add the all lexicon entries on this path to lexicons. The parameter
lexicons represents all sublexicons in the lexc file and their lexicon
entries.</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.LexcPath.get_lexc_paths"><code class="name flex">
<span>def <span class="ident">get_lexc_paths</span></span>(<span>self) ‑> list[list[<a title="ParserTools.src.lexc_path.LexcEntry" href="#ParserTools.src.lexc_path.LexcEntry">LexcEntry</a>]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_lexc_paths(self) -&gt; list[list[LexcEntry]]:
    &#34;&#34;&#34;Convert this path into a list of lexc lexicon paths starting
       at a person prefix lexicon (this could be VTA_Prefix, NA_Prefix, 
       etc.) and ending in the terminal lexicon #. Each path is a 
       sequence of lexc sublexicon entries.

       There will be one list-element for each surface form (note that 
       there may be several surface forms Form1Surface, Form2Surface, 
       ...).

       For inflected forms of regular lexemes, our paths will look
       like this (here, for the example analysis and intermediate form
       `aaba&#39;+VTA+Ind+Neg+Dub+0Pl+1Sg:ni&lt;&lt;aaba&#39;w&gt;&gt;igosiinaadogenan`):

       ```
          ! Person prefix lexicon for nouns and verbs. For all other 
          ! word classes the prefix is always empty (@P.Prefix.NONE@)
          LEXICON VTA_Prefix
          @P.Prefix.NI@:@P.Prefix.NI@ni VTA_PrefixBoundary ;

          ! Morpheme boundary for person prefix. Note that we can jump
          ! to a preverb lexicon here. For all word classes apart from
          ! nouns and verbs, we jump directly to the a stem lexicon.
          LEXICON VTA_PrefixBoundary
          0:%&lt;%&lt; PreverbRoot ;
    
          ! After adding preverbs, we return to this lexicon. Need to
          ! match the correct paradigm here
          LEXICON VerbStems
          @R.Paradigm.VTA@ VTA_Stems ;

          ! Stem lexicon. aaba&#39; belongs to the VTA_C inflection class,
          ! so we continue to the VTA_C suffix boundary lexicon.
          LEXICON VTA_Stems
          aaba&#39;:aaba&#39;w VTA_Class=VTA_C_Boundary ;

          ! Suffix boundary
          LEXICON VTA_Class=VTA_C_Boundary
          0:%&gt;%&gt; VTA_Class=VTA_C_Flags ;

          ! This sublexicon makes sure that we get the correct 
          ! combination of person prefix (&#34;ni-&#34; in this case) and ending.
          ! The combinatorics is handled by matching the value of the 
          ! feature Prefix (NI in this case).
          LEXICON VTA_Class=VTA_C_Flags
          @R.Prefix.NI@ VTA_Class=VTA_C_Flags_Prefix=NI ;

          ! We need to match the correct order here
          LEXICON VTA_Class=VTA_C_Flags_Prefix=NI ;
          @U.Order.Ind@ VTA_Class=VTA_C_Prefix=NI_Order=Ind_Endings ;

          ! This lexicon enumerates endings for the inflection class
          ! VAT_C which correspond to person prefix &#34;ni-&#34; and order Ind.
          LEXICON VTA_Class=VTA_C_Prefix=NI_Order=Ind_Endings
          +VTA+Ind+Neg+Dub+%0PlSubj+1SgObj:igosiinaadogenan # ;
       ```

       For inflected forms of irregular lexemes, our paths become
       very simple. We just enumerate the entire form as one
       chunk without morpheme boundaries. This effectively prevents any
       phonological rules from applying, which is exactly what we want
       for irregular lexemes:

       ```
          LEXICON ROOT
          VTA_Irregular ;

          LEXICON VTA_Irregular
          izhi+VTA+Ind+Pos+Neu+%0Pl+1Sg:nindigonan # ;
       ```
    &#34;&#34;&#34;
    paths = []
    paradigm = self.paradigm
    klass = self.klass
    for i, (surface, parts) in enumerate(self.forms):
        tags = deepcopy(self.tags)
        # Possibly append a +Alt tag to all analyses except the
        # first one.
        if i &gt; 0 and self.conf[&#34;append_alt_tag&#34;]:
            tags.append(ALT_TAG)
        if self.regular:
            # Initialize flag diacritics which:
            # (1) control combinations of person prefix and inflectional ending,
            # (2) check that we&#39;ve got the correct paradigm (this is needed to
            #     make sure that return to the correct paradigm after adding
            #     preverbs), and
            # (3) check the order (we track this because subordinate preverbs
            #     and the changed-conjunct marker require conjunct order and some
            #     preverbs have distinct independent and conjunct order surface
            #     forms) 
            set_prefix_flag, check_prefix_flag = LexcPath.get_prefix_flags(parts.prefix)
            _, check_paradigm_flag = LexcPath.get_paradigm_flags(paradigm)
            order, check_order_flag = self.get_order_flag()

            # Initialize the person prefix for this form
            prefix = &#34;NONE&#34; if parts.prefix == &#34;&#34; else parts.prefix.upper()

            # Initialize continuation lexicons needed on this path
            person_prefix_lexicon = f&#34;{paradigm}_Prefix&#34;
            morpheme_boundary_lexicon = f&#34;{paradigm}_PrefixBoundary&#34;
            preverb_lexicon = (self.conf[&#34;prefix_root&#34;] # This can also be the prenoun lexicon depending on paradigm
                               if &#34;prefix_root&#34; in self.conf
                               else None)
            pos_stem_lexicon = f&#34;{self.conf[&#39;pos&#39;]}Stems&#34; # E.g. VerbStems
            paradigm_stem_lexicon = f&#34;{paradigm}_Stems&#34; # E.g. VTA_Stems
            inflection_class_lexicon = f&#34;{paradigm}_Class={klass}_Boundary&#34;
            check_prefix_lexicon = f&#34;{paradigm}_Class={klass}_Flags&#34;
            check_order_lexicon = f&#34;{paradigm}_Class={klass}_Flags_Prefix={prefix}&#34;
            ending_lexicon = f&#34;{paradigm}_Class={klass}_Prefix={prefix}_Order={order}_Endings&#34;
            
            path = [
                LexcEntry(person_prefix_lexicon,
                          set_prefix_flag,
                          set_prefix_flag+parts.prefix,
                          morpheme_boundary_lexicon),
                LexcEntry(morpheme_boundary_lexicon,
                          &#34;0&#34;,
                          escape(PREFIX_BOUNDARY),
                          preverb_lexicon or pos_stem_lexicon),
                LexcEntry(pos_stem_lexicon,
                          check_paradigm_flag,
                          check_paradigm_flag,
                          paradigm_stem_lexicon),
                LexcEntry(paradigm_stem_lexicon,
                          self.lemma,
                          self.stem,
                          inflection_class_lexicon),
                LexcEntry(inflection_class_lexicon,
                          &#34;0&#34;,
                          escape(SUFFIX_BOUNDARY),
                          check_prefix_lexicon),
                LexcEntry(check_prefix_lexicon,
                          check_prefix_flag,
                          check_prefix_flag,
                          check_order_lexicon),
                LexcEntry(check_order_lexicon,
                          check_order_flag,
                          check_order_flag,
                          ending_lexicon),
                LexcEntry(ending_lexicon,
                          &#34;&#34;.join(tags),
                          parts.suffix,
                          &#34;#&#34;)
            ]
            paths.append(path)
        else:
            # Irregular forms are treated as one chunk and simply enumerated.
            paths.append([LexcEntry(f&#34;{paradigm}_Irregular&#34;,
                                    f&#34;{self.lemma}{&#39;&#39;.join(tags)}&#34;,
                                    surface,
                                    &#34;#&#34;)])

    return paths</code></pre>
</details>
<div class="desc"><p>Convert this path into a list of lexc lexicon paths starting
at a person prefix lexicon (this could be VTA_Prefix, NA_Prefix,
etc.) and ending in the terminal lexicon #. Each path is a
sequence of lexc sublexicon entries.</p>
<p>There will be one list-element for each surface form (note that
there may be several surface forms Form1Surface, Form2Surface,
&hellip;).</p>
<p>For inflected forms of regular lexemes, our paths will look
like this (here, for the example analysis and intermediate form
<code>aaba'+VTA+Ind+Neg+Dub+0Pl+1Sg:ni&lt;&lt;aaba'w&gt;&gt;igosiinaadogenan</code>):</p>
<pre><code>   ! Person prefix lexicon for nouns and verbs. For all other 
   ! word classes the prefix is always empty (@P.Prefix.NONE@)
   LEXICON VTA_Prefix
   @P.Prefix.NI@:@P.Prefix.NI@ni VTA_PrefixBoundary ;

   ! Morpheme boundary for person prefix. Note that we can jump
   ! to a preverb lexicon here. For all word classes apart from
   ! nouns and verbs, we jump directly to the a stem lexicon.
   LEXICON VTA_PrefixBoundary
   0:%&lt;%&lt; PreverbRoot ;

   ! After adding preverbs, we return to this lexicon. Need to
   ! match the correct paradigm here
   LEXICON VerbStems
   @R.Paradigm.VTA@ VTA_Stems ;

   ! Stem lexicon. aaba' belongs to the VTA_C inflection class,
   ! so we continue to the VTA_C suffix boundary lexicon.
   LEXICON VTA_Stems
   aaba':aaba'w VTA_Class=VTA_C_Boundary ;

   ! Suffix boundary
   LEXICON VTA_Class=VTA_C_Boundary
   0:%&gt;%&gt; VTA_Class=VTA_C_Flags ;

   ! This sublexicon makes sure that we get the correct 
   ! combination of person prefix (&quot;ni-&quot; in this case) and ending.
   ! The combinatorics is handled by matching the value of the 
   ! feature Prefix (NI in this case).
   LEXICON VTA_Class=VTA_C_Flags
   @R.Prefix.NI@ VTA_Class=VTA_C_Flags_Prefix=NI ;

   ! We need to match the correct order here
   LEXICON VTA_Class=VTA_C_Flags_Prefix=NI ;
   @U.Order.Ind@ VTA_Class=VTA_C_Prefix=NI_Order=Ind_Endings ;

   ! This lexicon enumerates endings for the inflection class
   ! VAT_C which correspond to person prefix &quot;ni-&quot; and order Ind.
   LEXICON VTA_Class=VTA_C_Prefix=NI_Order=Ind_Endings
   +VTA+Ind+Neg+Dub+%0PlSubj+1SgObj:igosiinaadogenan # ;
</code></pre>
<p>For inflected forms of irregular lexemes, our paths become
very simple. We just enumerate the entire form as one
chunk without morpheme boundaries. This effectively prevents any
phonological rules from applying, which is exactly what we want
for irregular lexemes:</p>
<pre><code>   LEXICON ROOT
   VTA_Irregular ;

   LEXICON VTA_Irregular
   izhi+VTA+Ind+Pos+Neu+%0Pl+1Sg:nindigonan # ;
</code></pre></div>
</dd>
<dt id="ParserTools.src.lexc_path.LexcPath.get_order_flag"><code class="name flex">
<span>def <span class="ident">get_order_flag</span></span>(<span>self) ‑> tuple[str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_order_flag(self) -&gt; tuple[str]:
    &#34;&#34;&#34;Get a U flag matching the order of this LexcPath: Ind, Cnj or
       Other (in case of imperative or other unspecified order).

       The flag diacritic will be added to `multichar_symbols`.
    &#34;&#34;&#34;
    order = &#34;Other&#34;
    if &#34;+Ind&#34; in self.tags:
        order = &#34;Ind&#34;
    elif &#34;+Cnj&#34; in self.tags:
        order = &#34;Cnj&#34;
    flag = f&#34;@U.Order.{order}@&#34;
    LexcPath.multichar_symbols.update([flag])
    return order, flag</code></pre>
</details>
<div class="desc"><p>Get a U flag matching the order of this LexcPath: Ind, Cnj or
Other (in case of imperative or other unspecified order).</p>
<p>The flag diacritic will be added to <code>multichar_symbols</code>.</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.LexcPath.harvest_multichar_symbols"><code class="name flex">
<span>def <span class="ident">harvest_multichar_symbols</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def harvest_multichar_symbols(self) -&gt; None:
    &#34;&#34;&#34;Add all morphological features like &#34;+VTA&#34;, &#34;+Ind&#34; and &#34;+1SgSubj&#34;
       from this path to the multichar_symbols set

    &#34;&#34;&#34;
    for tag in self.tags:
        LexcPath.multichar_symbols.add(tag)</code></pre>
</details>
<div class="desc"><p>Add all morphological features like "+VTA", "+Ind" and "+1SgSubj"
from this path to the multichar_symbols set</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.LexcPath.read_forms"><code class="name flex">
<span>def <span class="ident">read_forms</span></span>(<span>self, row: pandas.core.series.Series, conf: dict) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_forms(self, row:pd.core.series.Series, conf:dict) -&gt; None:
    &#34;&#34;&#34;Read all forms on the given dataframe row. Store both the plain
       surface form and segmented form.

    &#34;&#34;&#34;
    def get_form_indices() -&gt; list[int]:
        # Return all indices i which are associated with a surface
        # form on this row, i.e. i where Form{i}Surface is a
        # column on the row and the form in that column is
        # non-empty.
        missing = conf[&#34;missing_form_marker&#34;]
        return [i for i in range(MAXFORMS) if f&#34;Form{i}Surface&#34; in row and
                                              not row[f&#34;Form{i}Surface&#34;] in [missing, &#34;&#34;]]
    
    self.forms = [(row[f&#34;Form{i}Surface&#34;], split_form(row[f&#34;Form{i}Split&#34;]))
                  for i in get_form_indices()]
    if len(self.forms) == 0:
        raise ValueError(f&#34;No surface forms given for row: {row.to_dict()}&#34;)</code></pre>
</details>
<div class="desc"><p>Read all forms on the given dataframe row. Store both the plain
surface form and segmented form.</p></div>
</dd>
</dl>
</dd>
<dt id="ParserTools.src.lexc_path.SplitForm"><code class="flex name class">
<span>class <span class="ident">SplitForm</span></span>
<span>(</span><span>prefix, stem, suffix)</span>
</code></dt>
<dd>
<div class="desc"><p>SplitForm represents an inflected word form consisting of a
prefix, stem and suffix:</p>
<pre><code>&quot;pre&lt;&lt;st&gt;&gt;suf&quot;
</code></pre>
<p>corresponds to:</p>
<pre><code>split_form = SplitForm(&quot;pre&quot;,&quot;st&quot;,&quot;suf&quot;)
</code></pre>
<p>Access features as:</p>
<pre><code>split_form.prefix, split_form.stem, split_form.suffix
</code></pre></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="ParserTools.src.lexc_path.SplitForm.prefix"><code class="name">var <span class="ident">prefix</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.SplitForm.stem"><code class="name">var <span class="ident">stem</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
<dt id="ParserTools.src.lexc_path.SplitForm.suffix"><code class="name">var <span class="ident">suffix</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 2</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ParserTools.src" href="index.html">ParserTools.src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="ParserTools.src.lexc_path.ALT_TAG" href="#ParserTools.src.lexc_path.ALT_TAG">ALT_TAG</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.MAXFORMS" href="#ParserTools.src.lexc_path.MAXFORMS">MAXFORMS</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.PREFIX_BOUNDARY" href="#ParserTools.src.lexc_path.PREFIX_BOUNDARY">PREFIX_BOUNDARY</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.SUFFIX_BOUNDARY" href="#ParserTools.src.lexc_path.SUFFIX_BOUNDARY">SUFFIX_BOUNDARY</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ParserTools.src.lexc_path.entry2str" href="#ParserTools.src.lexc_path.entry2str">entry2str</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.escape" href="#ParserTools.src.lexc_path.escape">escape</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.split_form" href="#ParserTools.src.lexc_path.split_form">split_form</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ParserTools.src.lexc_path.DerivationPath" href="#ParserTools.src.lexc_path.DerivationPath">DerivationPath</a></code></h4>
<ul class="">
<li><code><a title="ParserTools.src.lexc_path.DerivationPath.extend_lexicons" href="#ParserTools.src.lexc_path.DerivationPath.extend_lexicons">extend_lexicons</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ParserTools.src.lexc_path.LexcEntry" href="#ParserTools.src.lexc_path.LexcEntry">LexcEntry</a></code></h4>
<ul class="">
<li><code><a title="ParserTools.src.lexc_path.LexcEntry.analysis" href="#ParserTools.src.lexc_path.LexcEntry.analysis">analysis</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.LexcEntry.lexicon" href="#ParserTools.src.lexc_path.LexcEntry.lexicon">lexicon</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.LexcEntry.next_lexicon" href="#ParserTools.src.lexc_path.LexcEntry.next_lexicon">next_lexicon</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.LexcEntry.surface" href="#ParserTools.src.lexc_path.LexcEntry.surface">surface</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ParserTools.src.lexc_path.LexcPath" href="#ParserTools.src.lexc_path.LexcPath">LexcPath</a></code></h4>
<ul class="">
<li><code><a title="ParserTools.src.lexc_path.LexcPath.extend_lexicons" href="#ParserTools.src.lexc_path.LexcPath.extend_lexicons">extend_lexicons</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.LexcPath.get_lexc_paths" href="#ParserTools.src.lexc_path.LexcPath.get_lexc_paths">get_lexc_paths</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.LexcPath.get_order_flag" href="#ParserTools.src.lexc_path.LexcPath.get_order_flag">get_order_flag</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.LexcPath.get_paradigm_flags" href="#ParserTools.src.lexc_path.LexcPath.get_paradigm_flags">get_paradigm_flags</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.LexcPath.get_prefix_flags" href="#ParserTools.src.lexc_path.LexcPath.get_prefix_flags">get_prefix_flags</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.LexcPath.harvest_multichar_symbols" href="#ParserTools.src.lexc_path.LexcPath.harvest_multichar_symbols">harvest_multichar_symbols</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.LexcPath.multichar_symbols" href="#ParserTools.src.lexc_path.LexcPath.multichar_symbols">multichar_symbols</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.LexcPath.read_forms" href="#ParserTools.src.lexc_path.LexcPath.read_forms">read_forms</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.LexcPath.update_multichar_symbol_set" href="#ParserTools.src.lexc_path.LexcPath.update_multichar_symbol_set">update_multichar_symbol_set</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ParserTools.src.lexc_path.SplitForm" href="#ParserTools.src.lexc_path.SplitForm">SplitForm</a></code></h4>
<ul class="">
<li><code><a title="ParserTools.src.lexc_path.SplitForm.prefix" href="#ParserTools.src.lexc_path.SplitForm.prefix">prefix</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.SplitForm.stem" href="#ParserTools.src.lexc_path.SplitForm.stem">stem</a></code></li>
<li><code><a title="ParserTools.src.lexc_path.SplitForm.suffix" href="#ParserTools.src.lexc_path.SplitForm.suffix">suffix</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
